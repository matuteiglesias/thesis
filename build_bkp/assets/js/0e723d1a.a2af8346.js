"use strict";(self.webpackChunkthesis=self.webpackChunkthesis||[]).push([[8654],{4007:(e,s,a)=>{a.r(s),a.d(s,{assets:()=>m,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"Aggregation/chapter1_55","title":"Appendix IV: Computational Experiments","description":"Detailed description of computational experiments for measuring dependence of aggregate idiosyncratic variance with population size.","source":"@site/docs/Aggregation/chapter1_55.md","sourceDirName":"Aggregation","slug":"/appendix-computational-experiments","permalink":"/docs/appendix-computational-experiments","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Aggregation/chapter1_55.md","tags":[{"inline":true,"label":"experiments","permalink":"/docs/tags/experiments"},{"inline":true,"label":"data analysis","permalink":"/docs/tags/data-analysis"},{"inline":true,"label":"Python","permalink":"/docs/tags/python"}],"version":"current","sidebarPosition":55,"frontMatter":{"sidebar_position":55,"slug":"/appendix-computational-experiments","title":"Appendix IV: Computational Experiments","description":"Detailed description of computational experiments for measuring dependence of aggregate idiosyncratic variance with population size.","keywords":["computational experiments","idiosyncratic variance","population size","Python","Jupyter Notebooks"],"tags":["experiments","data analysis","Python"]},"sidebar":"tutorialSidebar","previous":{"title":"Moments of a Log-Laplace","permalink":"/docs/moments-of-a-log-laplace"},"next":{"title":"Experiment 2: Power Sums","permalink":"/docs/experiment-power-sums"}}');var t=a(4848),i=a(8453);const r={sidebar_position:55,slug:"/appendix-computational-experiments",title:"Appendix IV: Computational Experiments",description:"Detailed description of computational experiments for measuring dependence of aggregate idiosyncratic variance with population size.",keywords:["computational experiments","idiosyncratic variance","population size","Python","Jupyter Notebooks"],tags:["experiments","data analysis","Python"]},l=void 0,m={},c=[{value:"Appendix IV: Computational Experiments",id:"appendix-iv-computational-experiments",level:2},{value:"Exp. 1. Dependence with N",id:"exp-1-dependence-with-n",level:3}];function o(e){const s={a:"a",annotation:"annotation",code:"code",h2:"h2",h3:"h3",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msubsup:"msubsup",ol:"ol",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.h2,{id:"appendix-iv-computational-experiments",children:"Appendix IV: Computational Experiments"}),"\n",(0,t.jsxs)(s.p,{children:["The codes for reproducing all results in this paper are available in a GitHub ",(0,t.jsx)(s.strong,{children:(0,t.jsx)(s.a,{href:"https://github.com/matuteiglesias/French_exports_bkp",children:"repository"})}),". The language used is Python, and files are Jupyter notebooks. Below is a description of each experiment performed, along with pseudo code."]}),"\n",(0,t.jsx)(s.h3,{id:"exp-1-dependence-with-n",children:"Exp. 1. Dependence with N"}),"\n",(0,t.jsxs)(s.p,{children:["This experiment aims to measure the dependence of aggregate idiosyncratic variance (",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsx)(s.mrow,{children:(0,t.jsxs)(s.msubsup,{children:[(0,t.jsx)(s.mi,{children:"\u03c3"}),(0,t.jsx)(s.mi,{children:"\u03f5"}),(0,t.jsx)(s.mn,{children:"2"})]})}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\sigma^2_\\epsilon"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"1.0611em",verticalAlign:"-0.247em"}}),(0,t.jsxs)(s.span,{className:"mord",children:[(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03c3"}),(0,t.jsx)(s.span,{className:"msupsub",children:(0,t.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(s.span,{className:"vlist-r",children:[(0,t.jsxs)(s.span,{className:"vlist",style:{height:"0.8141em"},children:[(0,t.jsxs)(s.span,{style:{top:"-2.453em",marginLeft:"-0.0359em",marginRight:"0.05em"},children:[(0,t.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(s.span,{className:"mord mathnormal mtight",children:"\u03f5"})})]}),(0,t.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,t.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(s.span,{className:"mord mtight",children:"2"})})]})]}),(0,t.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,t.jsx)(s.span,{className:"vlist-r",children:(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.247em"},children:(0,t.jsx)(s.span,{})})})]})})]})]})})]}),") with population size (",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsx)(s.mrow,{children:(0,t.jsx)(s.mi,{children:"N"})}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"N"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"N"})]})})]}),"). The process follows these steps:"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.strong,{children:"Prepare dataset"})}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsxs)(s.strong,{children:["Sample ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsx)(s.mrow,{children:(0,t.jsx)(s.mi,{children:"N"})}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"N"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"N"})]})})]})," agents with replacement"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Compute aggregate statistics."}),"\n",(0,t.jsx)(s.li,{children:"Apply random partition and compute parts' time series."}),"\n",(0,t.jsx)(s.li,{children:"Apply quantile partition and compute parts' time series."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.strong,{children:"Separate medians from idiosyncrasies"})}),"\n",(0,t.jsx)(s.li,{children:(0,t.jsx)(s.strong,{children:"Compute covariance matrices"})}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"For this exercise, we aim to create a dataset similar to the raw data, with the condition that there is no entry and exit. The steps are:"}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsx)(s.li,{children:"Keep only firms active in at least 6 of the total 17 years available."}),"\n",(0,t.jsx)(s.li,{children:"Fill inactive years with the mean value of the respective firms."}),"\n",(0,t.jsxs)(s.li,{children:["Retain only firms trading more than ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsxs)(s.mrow,{children:[(0,t.jsx)(s.mn,{children:"1"}),(0,t.jsx)(s.mi,{children:"m"}),(0,t.jsx)(s.mi,{children:"E"}),(0,t.jsx)(s.mi,{children:"U"}),(0,t.jsx)(s.mi,{children:"R"})]}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"1m EUR"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,t.jsx)(s.span,{className:"mord",children:"1"}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"m"}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05764em"},children:"E"}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"U"}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.00773em"},children:"R"})]})})]})," on average."]}),"\n"]}),"\n",(0,t.jsxs)(s.p,{children:["These changes result in a dataset with a size distribution similar to the upper tail (Pareto part) of the original data. Firms trading more than ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsxs)(s.mrow,{children:[(0,t.jsx)(s.mn,{children:"1"}),(0,t.jsx)(s.mi,{children:"m"}),(0,t.jsx)(s.mi,{children:"E"}),(0,t.jsx)(s.mi,{children:"U"}),(0,t.jsx)(s.mi,{children:"R"})]}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"1m EUR"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,t.jsx)(s.span,{className:"mord",children:"1"}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"m"}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05764em"},children:"E"}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"U"}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.00773em"},children:"R"})]})})]})," on average are present in most time steps, so the n/a filling is minimal. Additionally, the largest firms concentrate most of the value, so retaining only those above ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsxs)(s.mrow,{children:[(0,t.jsx)(s.mn,{children:"1"}),(0,t.jsx)(s.mi,{children:"m"}),(0,t.jsx)(s.mi,{children:"E"}),(0,t.jsx)(s.mi,{children:"U"}),(0,t.jsx)(s.mi,{children:"R"})]}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"1m EUR"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,t.jsx)(s.span,{className:"mord",children:"1"}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"m"}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05764em"},children:"E"}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"U"}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.00773em"},children:"R"})]})})]})," still accounts for nearly all of French firms' international trade."]}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:"# Number of parts:\nQ = 10\n\n# Load data, sales by firm, year.\ndf = pd.read_csv('./ID_Y.csv')\nsales = df.groupby(['IMPORT', 'ID', 'YEAR'])['VART'].sum().unstack()\n\nfor i in [0, 1]:\n\n    # Choose firms with presence in most samples to avoid high distortion filling exit gaps\n    sales_filt = sales.loc[sales.count(1) > 6]\n    filt_fm = sales_filt.copy()\n\n    # Large firms pareto filled mean. \n    for col in filt_fm:\n        filt_fm[col] = filt_fm[col].fillna(sales_filt.mean(axis=1))\n\n    # Hard cut for Pareto tail\n    filt_fm = filt_fm.loc[filt_fm.mean(1) > 1e6]\n    \n    ## Sanity checks. What is the total after we filled non-active gaps and kept large firms\n    X = sales.sum().mean()\n    X_actives = sales_filt.sum().mean()\n    X_act_lrg = filt_fm.sum().mean()\n\n    print(X_actives/X)\n    print(X_act_lrg/X_actives)\n    print(X_act_lrg/X)\n    \n# Save dataset\nfilt_fm.to_csv('./firms_data.csv')\n"})}),"\n",(0,t.jsx)(s.p,{children:"The processed dataset is always within 10 percent of the total observed in the raw data."}),"\n",(0,t.jsxs)(s.p,{children:["For the experiment, we will sample ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsx)(s.mrow,{children:(0,t.jsx)(s.mi,{children:"N"})}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"N"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"N"})]})})]})," agents with replacement and compute aggregate statistics, apply partitions, and compute the cross covariance matrices these present."]}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:"# Population numbers\nlogn_vals = [2.5, 2.65, ..., 3.7, 3.85] # log scale\nn_vals = [300., 400., ..., 5000., 7100.] # linear scale\n\n# Repetitions\nM = 150\n\ndata = pd.read_csv('./firms_data.csv')\n\nfor i in [0, 1]: # Exports / Imports\n    for N in n_vals:\n        for m in range(M):\n\n            # Sample with replacement from agents' time series\n            n_sample = data.sample(int(N), replace=True)\n            \n            ## Calculate aggregate magnitudes: Total, firm sizes, Herfindahl.\n            X_t = n_sample.sum()\n            Si = n_sample.mean(1)\n            herf2 = ((Si/Si.sum())**2).sum()\n            agg_res += [[m, X_t.mean(), X_t.std(), np.log10(X_t).std(), herf2]]\n            \n            # Partition (random parts) \n            n_sample_p = n_sample.copy()\n            n_sample_p['p'] = pd.cut(n_sample_p.sum(1).cumsum(), Q, labels=range(Q))\n\n            # Aggregate to parts' time series, and count parts' population.\n            n_m_p_out = n_sample_p.groupby('p').sum().reset_index()\n            n_m_p_out['n'] = n_sample_p.groupby('p').size().values\n            \n            # Partition (quantile parts) \n            n_sample_q = n_sample.copy() \n            n_sample_q = n_sample_q.loc[n_sample_q.sum(1).sort_values().index]  ## SORTING\n            n_sample_q['p'] = pd.cut(n_sample_q.sum(1).cumsum(), Q, labels=range(Q))\n\n            # Aggregate to parts' time series, and count parts' population.\n            n_m_q_out = n_sample_q.groupby('p').sum().reset_index()\n            n_m_q_out['n'] = n_sample_q.groupby('p').size().values    \n\n            <Store results>\n            \n<Concatenate results and save>\nresult_aggs\nresult_Sp\nresult_Sq\n"})}),"\n",(0,t.jsxs)(s.p,{children:["Next, we compute medians across the ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsx)(s.mrow,{children:(0,t.jsx)(s.mi,{children:"M"})}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"M"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"M"})]})})]})," repetitions and use it as a proxy for the comovement time series of parts."]}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:"# Store medians across M repetitions\nmedians_p = result_Sp.groupby(['IMPORT', 'N', 'p']).transform('median'); \n\n# Idiosyncrasies are the actual values minus the medians\nres_nmp = result_Sp.set_index(['IMPORT','N', 'p']) - medians_p; \n\n# Store info\ninfo_p = pd.concat([medians_p, res_nmp])\n\n<Repeat for result_Sq in place of result_Sp>\n"})}),"\n",(0,t.jsxs)(s.p,{children:["Finally, compute the values of the cross covariance matrix for each setting and each of the ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsx)(s.mrow,{children:(0,t.jsx)(s.mi,{children:"M"})}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"M"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"M"})]})})]})," repetitions."]}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:"cov_out_list = [] # List for outcoming cov values\n\nfor i in [0, 1]: # Exports / Imports\n    for N in n_vals: # For all the sampling sizes (population N)\n        for m in range(M): # For each of the repetitions\n            for k, sorting in enumerate([False, True]): # For both random and sorted parts\n                info = [info_p, info_q][k]\n                df_ = info.loc[(info.IMPORT == i) & (info.m == m) & (info['N'] == N)]\n                df_ = df_.set_index(['comp', 'p'])[[str(y) for y in range(1997, 2013)]]\n                cov_m = df_.T.cov()\n\n                cov_vals = cov_m.stack([0, 1])\n                cov_vals.index.names = ['comp1', 'p1', 'comp2', 'p2']; \n                cov_vals.name = 'cov_ij'\n\n                # Store\n                cov_out_list += [cov_vals]\n            \n# Concatenate\ncov_results = pd.concat(cov_out_list)\n"})}),"\n",(0,t.jsxs)(s.p,{children:["Now, we have ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsx)(s.mrow,{children:(0,t.jsx)(s.mi,{children:"M"})}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"M"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"M"})]})})]})," (e.g., ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsxs)(s.mrow,{children:[(0,t.jsx)(s.mi,{children:"M"}),(0,t.jsx)(s.mo,{children:"="}),(0,t.jsx)(s.mn,{children:"150"})]}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"M = 150"})]})})}),(0,t.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"M"}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,t.jsx)(s.span,{className:"mrel",children:"="}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,t.jsx)(s.span,{className:"mord",children:"150"})]})]})]}),") realizations of each element of the cross covariance matrix, separated into median (comovement) and residual (idiosyncratic) parts, for different population sizes ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsx)(s.mrow,{children:(0,t.jsx)(s.mi,{children:"N"})}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"N"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.10903em"},children:"N"})]})})]})," and both for imports and exports. This information has multiple uses, such as creating the plots in ",(0,t.jsx)(s.strong,{children:"Figure N_decay_P"}),"."]})]})}function h(e={}){const{wrapper:s}={...(0,i.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(o,{...e})}):o(e)}},8453:(e,s,a)=>{a.d(s,{R:()=>r,x:()=>l});var n=a(6540);const t={},i=n.createContext(t);function r(e){const s=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),n.createElement(i.Provider,{value:s},e.children)}}}]);